{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "550d637a-d22d-4fdd-9562-df5a67b40701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a72309-7f5c-4626-8a80-ed0d1da0c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('board_grade.npy', allow_pickle=True)\n",
    "x, y = data[:, 1], data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6368edfd-466c-4ecc-a648-62dde762d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67d2222-9332-42b3-b44e-aaa09e173948",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boards = np.stack(x).reshape(-1, 18, 11, 2)\n",
    "all_classes = np.stack(y).reshape(-1, 7)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_tensor = torch.tensor(all_boards, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(all_classes, dtype=torch.float32)\n",
    "# create the dataset\n",
    "dataset = data.TensorDataset(x_tensor, y_tensor)\n",
    "val_size = int(len(dataset)*0.2)\n",
    "train_size = len(dataset)- int(len(dataset)*0.2)\n",
    "train_dataset, val_dataset = data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15e7525b-6331-468b-9457-43f491d36bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 18 * 11 * 2\n",
    "LATENT_DIM = 40\n",
    "\n",
    "class MLP_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, 160)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden_fc = nn.Linear(160, 80)\n",
    "        self.mean_fc = nn.Linear(80, latent_dim)\n",
    "        self.var_fc = nn.Linear(80, latent_dim)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_1 = F.mish(self.input_fc(x))\n",
    "        h_2 = F.mish(self.hidden_fc(h_1))\n",
    "        y_mean = self.mean_fc(h_2)\n",
    "        y_var = self.var_fc(h_2)\n",
    "        return y_mean, y_var\n",
    "\n",
    "class MLP_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(latent_dim, 80)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden_fc = nn.Linear(80, 160)\n",
    "        self.output_fc = nn.Linear(160, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_1 = F.mish(self.input_fc(x))\n",
    "        h_2 = F.mish(self.hidden_fc(h_1))\n",
    "        x_new = torch.sigmoid(self.output_fc(h_2))\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ced29e7-b707-4f85-9f34-dc69de6d2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super().__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6dc8310-95cb-458d-890d-27ddafb21517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cdd2082a-d7cc-4643-a077-482520000815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 23.029 | Val. Loss: 23.224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 23.043 | Val. Loss: 23.144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 23.029 | Val. Loss: 23.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 22.970 | Val. Loss: 23.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 23.004 | Val. Loss: 23.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 23.008 | Val. Loss: 23.144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 22.951 | Val. Loss: 23.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 22.961 | Val. Loss: 23.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     69\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m---> 71\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_iterator, device)\n\u001b[0;32m     74\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n",
      "Cell \u001b[1;32mIn[71], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m x_hat, mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(x, x_hat, mean, log_var)\n\u001b[0;32m     39\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[66], line 14\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterization(mean, torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m log_var)) \u001b[38;5;66;03m# takes exponential function (log var -> var)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDecoder(z)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[65], line 17\u001b[0m, in \u001b[0;36mMLP_Encoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     h_1 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmish\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     h_2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmish(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_fc(h_1))\n\u001b[0;32m     19\u001b[0m     y_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_fc(h_2)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2091\u001b[0m, in \u001b[0;36mmish\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   2090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmish_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 2091\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmish\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "\n",
    "encoder = MLP_Encoder(input_dim=INPUT_DIM, latent_dim=LATENT_DIM)\n",
    "decoder = MLP_Decoder(latent_dim=LATENT_DIM, output_dim=INPUT_DIM)\n",
    "\n",
    "model = VAE(Encoder=encoder, Decoder=decoder).to(device)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_iterator = data.DataLoader(train_dataset,\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(val_dataset,\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "def train(model, iterator, optimizer, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, _) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(x, x_hat, mean, log_var)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss / (len(iterator) * BATCH_SIZE)\n",
    "\n",
    "def evaluate(model, iterator, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, _) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = x.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / (len(iterator) * BATCH_SIZE)\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, device)\n",
    "    valid_loss = evaluate(model, valid_iterator, device)\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6742b900-d2c0-4451-ac86-2a17ab4e3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board_bar():\n",
    "    for i in range(26):\n",
    "        print(\"-\", end=\"\")\n",
    "    print()\n",
    "\n",
    "def print_board(board):\n",
    "    print(\" \"*3, end=\"| \")\n",
    "    for i in range(11):\n",
    "        print(f\"{chr(i + 65)}\", end=\" \")\n",
    "    print()\n",
    "    print_board_bar()\n",
    "    for i in range(18):\n",
    "        print(f\"{18 - i:2} |\", end=\" \")\n",
    "        for j in range(11):\n",
    "            num_printed = False\n",
    "            for c in range(2):\n",
    "                if board[17 - i, j, c] > 0:\n",
    "                    char = \"M\"\n",
    "                    if c > 0:\n",
    "                        char = \"S\" if i >= 12 else \"E\"\n",
    "                    print(char, end=\" \")  \n",
    "                    num_printed = True\n",
    "            if not num_printed:\n",
    "                print(\"-\", end=\" \")\n",
    "        print()\n",
    "    print_board_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "427ae21a-02c4-4bbd-a946-1fa212bd1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives us a map of holds for a given problem, which we can tweak to produce a board\n",
    "with torch.no_grad():\n",
    "    noise = torch.normal(0, 1, size=(1, LATENT_DIM)).to(device)\n",
    "    generated_board_map = decoder(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5cbc0964-8470-406a-9194-9152c9f33802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this uses the problem map to generate a board problem\n",
    "def build_problem_from_map(generated_board_map, num_middle_holds, num_start_holds, num_end_holds):\n",
    "    generated_board_map = generated_board_map.view(18, 11, 2)\n",
    "    board = np.zeros((18, 11, 2))\n",
    "    # first get middle holds\n",
    "    v, i = torch.topk(generated_board_map[:, :, 0].flatten(), num_middle_holds)\n",
    "    idx_list = np.array(np.unravel_index(i.cpu().numpy(), (18, 11))).T\n",
    "    for[x, y] in idx_list:\n",
    "        board[x, y, 0] = 1\n",
    "    # now get start hold(s)\n",
    "    v, i = torch.topk(generated_board_map[:6, :, 1].flatten(), num_start_holds)\n",
    "    idx_list = np.array(np.unravel_index(i.cpu().numpy(), (6, 11))).T\n",
    "    for[x, y] in idx_list:\n",
    "        board[x, y, 1] = 1\n",
    "    # then end hold(s)\n",
    "    v, i = torch.topk(generated_board_map[17:, :, 1].flatten(), num_end_holds)\n",
    "    idx_list = np.array(np.unravel_index(i.cpu().numpy(), (1, 11))).T\n",
    "    for[x, y] in idx_list:\n",
    "        board[x + 17, y, 1] = 1\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "93ff9127-50bd-4d7d-b127-101df76942e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | A B C D E F G H I J K \n",
      "--------------------------\n",
      "18 | - - - - - - E - - - - \n",
      "17 | - - - - - - - - - - - \n",
      "16 | - - - - - - - - - - - \n",
      "15 | - - - - M - - - - - - \n",
      "14 | - - - - - - - - - - - \n",
      "13 | - - - - - - M - - - - \n",
      "12 | - - - - - - - - - - - \n",
      "11 | - - - - - - - - - - - \n",
      "10 | - - - - M - - M - - - \n",
      " 9 | - - - - - - - - - - - \n",
      " 8 | - - - - - - - - - - - \n",
      " 7 | - - - - - - - - - - - \n",
      " 6 | - - - - - - - - - - - \n",
      " 5 | - - - - - S - - - - - \n",
      " 4 | - - - - - - S - - - - \n",
      " 3 | - - - - - - - - - - - \n",
      " 2 | - - - - - - - - - - - \n",
      " 1 | - - - - - - - - - - - \n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# the full problem generator\n",
    "with torch.no_grad():\n",
    "        noise = torch.normal(0, 1, size=(1, LATENT_DIM)).to(device)\n",
    "        generated_board_map = decoder(noise)\n",
    "gen_board = build_problem_from_map(generated_board_map, 4, 2, 1)\n",
    "print_board(gen_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ccf9c-73bd-4f47-a422-7f26ba8190b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a8c07-41a7-429e-921d-92eac579e9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72105e-b111-4ec1-9471-38f8ba350679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
